{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16672,"status":"ok","timestamp":1751216525413,"user":{"displayName":"Marcin Kubacki","userId":"14972583737076123112"},"user_tz":-120},"id":"3776s2ncl79J","outputId":"a71fa2ae-336c-4d41-cb7d-f7d7e0e7e796"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Collecting datasets\n","  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n","Collecting huggingface_hub\n","  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n","Collecting fsspec\n","  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fsspec, huggingface_hub, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: huggingface_hub\n","    Found existing installation: huggingface-hub 0.33.0\n","    Uninstalling huggingface-hub-0.33.0:\n","      Successfully uninstalled huggingface-hub-0.33.0\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.14.4\n","    Uninstalling datasets-2.14.4:\n","      Successfully uninstalled datasets-2.14.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0 huggingface_hub-0.33.1\n"]}],"source":["!pip install datasets transformers --quiet\n","!pip install datasets --quiet\n","!pip install -U datasets huggingface_hub fsspec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jn-Mhg2ir2lD"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","import re\n","import json\n","import glob\n","from tqdm.auto import tqdm\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16954,"status":"ok","timestamp":1751216558670,"user":{"displayName":"Marcin Kubacki","userId":"14972583737076123112"},"user_tz":-120},"id":"BqVxHiGPzQf7","outputId":"aab30e9f-b228-42e8-bfe9-a316c6bd7e77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","batch_size = 10"],"metadata":{"id":"MV37gmY-q9IE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmwpqclazF8K"},"outputs":[],"source":["def calculate_mask_attribution(texts, labels, k=15, save_path=None, debug=False):\n","    results = []\n","\n","    for idx, (text, label) in enumerate(zip(texts, labels)):\n","        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n","        token_ids = inputs[\"input_ids\"]\n","        attention_mask = inputs[\"attention_mask\"]\n","        tokens = tokenizer.convert_ids_to_tokens(token_ids[0])\n","\n","        with torch.no_grad():\n","            original_logits = model(**inputs).logits\n","        original_conf = torch.softmax(original_logits, dim=-1)[0].max().item()\n","\n","        attributions = []\n","        for i in range(1, token_ids.size(1) - 1):\n","            perturbed_ids = token_ids.clone()\n","            perturbed_ids[0, i] = tokenizer.mask_token_id if tokenizer.mask_token_id else tokenizer.unk_token_id\n","            with torch.no_grad():\n","                perturbed_logits = model(input_ids=perturbed_ids, attention_mask=attention_mask).logits\n","            perturbed_conf = torch.softmax(perturbed_logits, dim=-1)[0].max().item()\n","            attributions.append(original_conf - perturbed_conf)\n","\n","        valid_tokens = tokens[1:-1]\n","        valid_scores = attributions[:len(valid_tokens)]\n","        top_k = min(k, len(valid_tokens))\n","\n","        top_indices = sorted(range(len(valid_scores)), key=lambda i: abs(valid_scores[i]), reverse=True)[:top_k]\n","        top_tokens_scores = [(valid_tokens[i], round(valid_scores[i], 4)) for i in top_indices]\n","        summary_str = \", \".join([f\"{t} ({s:+.2f})\" for t, s in top_tokens_scores])\n","\n","        results.append({\n","            \"id\": idx,\n","            \"text\": text.strip(),\n","            \"label\": \"Positive\" if label else \"Negative\",\n","            \"tokens\": [tok for tok, _ in top_tokens_scores],\n","            \"weights\": [score for _, score in top_tokens_scores],\n","            \"token_positions\": top_indices\n","        })\n","\n","        if debug:\n","            print(f\"Original text:\\n{text.strip()}\\nSentiment: {'Positive' if label else 'Negative'}\")\n","            print(\"Top influential tokens:\")\n","            for tok, score in top_tokens_scores:\n","                print(f\"{tok:15} | weight: {score:+.4f}\")\n","            print(\"-\" * 80)\n","\n","    if save_path:\n","        df = pd.DataFrame(results)\n","        df.to_parquet(save_path, index=False)\n","        print(f\"Saved results to {save_path}\")\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwf9uv3SzIQ9"},"outputs":[],"source":["def gradient_attribution_sentiment(texts, labels, k=15, debug=False, save_path=None):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    results = []\n","\n","    for idx, (text, label) in enumerate(zip(texts, labels)):\n","        encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512,\n","                             return_offsets_mapping=True).to(device)\n","        input_ids = encoding.input_ids\n","        attention_mask = encoding.attention_mask\n","        offsets = encoding['offset_mapping'][0].tolist()\n","        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n","\n","        if hasattr(model, \"distilbert\"):\n","            embedding_layer = model.distilbert.embeddings.word_embeddings\n","        elif hasattr(model, \"roberta\"):\n","            embedding_layer = model.roberta.embeddings.word_embeddings\n","        elif hasattr(model, \"bert\"):\n","            embedding_layer = model.bert.embeddings.word_embeddings\n","        elif hasattr(model, \"transformer\") and hasattr(model.transformer, \"wte\"):\n","            embedding_layer = model.transformer.wte  # GPT-2\n","        elif hasattr(model, \"model\") and hasattr(model.model, \"encoder\") and hasattr(model.model.encoder, \"embed_tokens\"):\n","            embedding_layer = model.model.encoder.embed_tokens\n","        else:\n","            raise ValueError(\"Model architecture not supported for gradient attribution.\")\n","        inputs_embeds = embedding_layer(input_ids).detach().requires_grad_(True)\n","        outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        predicted_label = logits.argmax(dim=-1).item()\n","        target_logit = logits[0, predicted_label]\n","        target_logit.backward()\n","        grads = inputs_embeds.grad.abs().sum(dim=-1).squeeze()\n","\n","        valid_tokens = tokens[1:-1]\n","        valid_scores = grads[1:-1].tolist()\n","\n","        top_k = min(k, len(valid_tokens))\n","        sorted_idx = sorted(range(len(valid_scores)), key=lambda i: abs(valid_scores[i]), reverse=True)[:top_k]\n","        influential_tokens = [(valid_tokens[i], round(valid_scores[i], 4)) for i in sorted_idx]\n","        summary_str = \", \".join([f\"{t} ({s:+.2f})\" for t, s in influential_tokens])\n","\n","        results.append({\n","            \"id\": idx,\n","            \"text\": text.strip(),\n","            \"label\": \"Positive\" if label else \"Negative\",\n","            \"tokens\": [tok for tok, _ in influential_tokens],\n","            \"weights\": [score for _, score in influential_tokens],\n","            \"token_positions\": sorted_idx\n","        })\n","\n","        if debug:\n","            print(f\"Original text:\\n{text.strip()}\\nSentiment: {'Positive' if label else 'Negative'}\")\n","            print(\"Top influential tokens:\")\n","            for tok, score in influential_tokens:\n","                print(f\"{tok:15} | weight: {score:+.4f}\")\n","            print(\"-\" * 80)\n","\n","    if save_path:\n","        import pandas as pd\n","        df = pd.DataFrame(results)\n","        df.to_parquet(save_path, index=False)\n","        print(f\"Saved results to {save_path}\")\n","\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"97OI5Nu8zJsz"},"outputs":[],"source":["def perturb_text(text, num_samples=30, keep_prob=0.8):\n","    words = text.split()\n","    samples = []\n","    masks = []\n","    for _ in range(num_samples):\n","        mask = np.random.binomial(1, keep_prob, size=len(words)).astype(bool)\n","        sample = [w if keep else \"\" for w, keep in zip(words, mask)]\n","        samples.append(\" \".join(sample))\n","        masks.append(mask.astype(int))\n","    return samples, masks, words\n","\n","def predict_fn(texts):\n","    all_probs = []\n","    for text in texts:\n","        enc = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","        with torch.no_grad():\n","            logits = model(**enc).logits\n","            probs = torch.softmax(logits, dim=-1)\n","            all_probs.append(probs[0, 1].item())\n","    return torch.tensor(all_probs)\n","\n","class SurrogateModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.linear = nn.Linear(input_dim, 1, bias=True)\n","\n","    def forward(self, x):\n","        return torch.sigmoid(self.linear(x)).squeeze(1)\n","\n","def linear_surrogate(texts, labels, k=15, debug=False, save_path=None):\n","    results = []\n","\n","    for text, label in zip(texts, labels):\n","        samples, masks, words = perturb_text(text)\n","        masks = torch.tensor(masks).float().to(device)\n","        preds = predict_fn(samples).to(device)\n","\n","\n","        model_surr = SurrogateModel(input_dim=len(words)).to(device)\n","        optimizer = torch.optim.Adam(model_surr.parameters(), lr=0.01)\n","        loss_fn = nn.BCELoss()\n","\n","        for _ in range(25):\n","            optimizer.zero_grad()\n","            pred = model_surr(masks)\n","            loss = loss_fn(pred, preds)\n","            loss.backward()\n","            optimizer.step()\n","\n","        with torch.no_grad():\n","            weights = model_surr.linear.weight[0]\n","            top_k = min(k, len(words))\n","            sorted_idx = torch.topk(torch.abs(weights), top_k).indices\n","\n","            influential_tokens = [(words[i], round(weights[i].item(), 4)) for i in sorted_idx]\n","\n","            results.append({\n","                \"text\": text.strip(),\n","                \"label\": \"Positive\" if label else \"Negative\",\n","                \"tokens\": [tok for tok, _ in influential_tokens],\n","                \"weights\": [score for _, score in influential_tokens],\n","                \"token_positions\": sorted_idx.cpu().numpy().tolist()\n","            })\n","\n","            if debug:\n","                print(f'Original text:\\n{text.strip()}\\nSentiment: {\"Positive\" if label else \"Negative\"}')\n","                print(\"Top influential tokens:\")\n","                for tok, score in influential_tokens:\n","                    print(f\"{tok:15} | weight: {score:+.4f}\")\n","                print(\"-\" * 80)\n","\n","    if save_path:\n","        df = pd.DataFrame(results)\n","        df.to_parquet(save_path, index=False)\n","        print(f\"Saved results to {save_path}\")\n","\n","    return results"]},{"cell_type":"code","source":["def evidence_string_dropout_sentiment(\n","    texts,\n","    evidences,\n","    labels=None,\n","    debug=False,\n","    save_path=None,\n","    fname=\"eraser_string_drop.parquet\",\n","):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device).eval()\n","\n","    softmax = torch.nn.Softmax(dim=-1)\n","    results = []\n","\n","    if save_path:\n","        os.makedirs(save_path, exist_ok=True)\n","\n","    for idx, (txt, ev_list) in enumerate(zip(texts, evidences)):\n","        dropped_txt = txt\n","        for phrase in ev_list:\n","            pattern = re.compile(re.escape(phrase), flags=re.IGNORECASE)\n","            dropped_txt = pattern.sub(\"\", dropped_txt)\n","        dropped_txt = re.sub(r\"\\s{2,}\", \" \", dropped_txt).strip()\n","\n","        for variant, sent in [(\"full\", txt), (\"dropped\", dropped_txt)]:\n","            enc = tokenizer(\n","                sent,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                padding=True,\n","                max_length=512,\n","            ).to(device)\n","            with torch.no_grad():\n","                logits = model(**enc).logits\n","            prob_pos = softmax(logits)[0, 1].item()\n","            if variant == \"full\":\n","                prob_full = prob_pos\n","            else:\n","                prob_drop = prob_pos\n","\n","        delta = prob_full - prob_drop\n","\n","        res = dict(\n","            id=idx,\n","            text_full=txt.strip(),\n","            text_dropped=dropped_txt,\n","            prob_full=round(prob_full, 4),\n","            prob_dropped=round(prob_drop, 4),\n","            delta=round(delta, 4),\n","            evidences=ev_list,\n","            label=(\"Positive\" if labels and labels[idx] else \"Negative\") if labels else None,\n","        )\n","        results.append(res)\n","\n","        if debug:\n","            print(f\"[{idx}] dprob={delta:+.4f}\")\n","            if labels:\n","                print(\"Label:\", res[\"label\"])\n","            print(\"Removed phrases:\", ev_list)\n","            print(\"-\" * 80)\n","\n","    if save_path:\n","        parquet_path = os.path.join(save_path, fname)\n","        pd.DataFrame(results).to_parquet(parquet_path, index=False)\n","        if debug:\n","            print(f\"Wrote {len(results)} rows → {parquet_path}\")\n","\n","    return results"],"metadata":{"id":"3wTTDhTZkG9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383,"referenced_widgets":["10bd61e09a5a4a96af3e59a338caaffa","e84e8428a566438b8deaa1215446dd44","cb612e76c64e4d9c9bce44fda350e766","c9a05e1028d1461ca8067576bf305f23","869d06ff455c47d2a3b2f4129a357312","2abdbc2c05ae47cd883fb03d0616eedd","040d631ad49e42a08aea956d759356b6","a102703903f445a7838791daf452adaa","787cb2d18e0548109685f63c0a6fd50c","57e46e2641814397af20d41d32cfe41f","e30b4888a73f4d9aaf5e9a2935a68fd8","dd58f3b4d88f48ee92f65fa385a97d78","6c08792c5b5c4b87941b28894c0cc3f3","2b4e2e9ca11443e5a4f96d64f4d16b95","789ecf0c14e8462cabcc86a688b6d0b2","01083b8ee6de4606afde6784444b35fe","69a0fdfb1164450ba3f181ce555a8fa4","cb4e16a79d6e4beca0d8da85a46a774e","47b2489e7e074f5698f1653ecc77fe3c","a8482c31c8d64c0c980476fa94c437cd","ec64838d039a4d12bebe287d49e48f8f","bf71a2b01c7c44888af641aa71d385ab","da6a5fbf429045309dda967606a11a0b","c33a54a4863a431a90412dde9528d5f1","89303d9a5d0a41a8bdfc28e6183b38f7","1c2464d0e6e24aeca412172be61b5f32","8498e3178f1c44619cd614010e14c767","5f963062566d4c39b04269676b729134","aa72d82768b246869f57d194f0b7531a","0ee0ab4deb5e4479bb8f2b9bbaeb10a2","b35045127f784705bb9ebeaa077af4a1","748abe5d4e7145c1a37ef9f6dbaa7001","49ca9a40a79d42569e75efc8b437ac6d","6258faae35b9460391b29be4a59505be","566229e310aa404cb58106d07fd353b0","e7d3016a8bb64c7f905938e0d09b8aad","0ab851ea3c124a4a8b6884d8fb2ec6e5","308b1821bcb542bf8a8e446549b16375","b9f477ccfc5942b295d4ed3556a5cd7e","6b70c6ee8a8742f1bd2639590a93aa31","f4989deeda2443e3881a8cd97804dd0d","8841b4c11d0d40839959610d6f672571","e5f77d8c7b0d4340b706e68b75ab3544","441ef4548f1d4438b48e3dbeace79b08","4ea3fbde57c949f2923397efece5679c","a4063eb61c054109965414ecd9d076fd","ed68c75460f345cd8b6c4e9c2e762a1f","1b25ade8a1df40458486ed1a45b10e3c","9de143c9441441b9940832606ef1b015","6b374460a48749839a3e4f1724ad6906","9efaadf8fa0243feb085eae9d6a2d561","33a8ac4de7874b92a3b582ab5efdad1c","78ff3342e7094779b1b27b654ff5c899","f3adc412200a416f8c33c613751691d8","56d132bb75ab43be8d8c1b2e1b501ad5","ac6b8264005a492d8a9c2783458794b2","a6e6ccd97244481ab01859650cfa8c41","55378599df2a4f1fab43b5d4ccf25091","af191d3b2b5340c7988d4d4646c5f87e","ac1da6ac3a3e425692cc74092e2b8f04","63f8d7b4a118482e92c4c050073c0883","e21b8473ed8943c5bd307d1381738845","f537e1b0b0ae4e869f5d664ca40494d2","fafeb3253cf94fa1842872797f0324f0","e1d26c44cb78459187564018cd046d91","4ac2986597964ba99fb73ed21bcac3ef"]},"executionInfo":{"elapsed":6676,"status":"ok","timestamp":1751216575220,"user":{"displayName":"Marcin Kubacki","userId":"14972583737076123112"},"user_tz":-120},"id":"uW0r37FQzTYD","outputId":"36f99492-1308-42e7-803b-595dc1798ef4"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10bd61e09a5a4a96af3e59a338caaffa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["movie_rationales.py: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd58f3b4d88f48ee92f65fa385a97d78"}},"metadata":{}},{"name":"stdout","output_type":"stream","text":["The repository for movie_rationales contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/movie_rationales.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da6a5fbf429045309dda967606a11a0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/1600 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6258faae35b9460391b29be4a59505be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/200 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea3fbde57c949f2923397efece5679c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/199 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac6b8264005a492d8a9c2783458794b2"}},"metadata":{}}],"source":["eraser = load_dataset(\"movie_rationales\")\n","eraser_train = eraser[\"train\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6fMFRZQ0GZh","colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"status":"error","timestamp":1751183688267,"user_tz":-120,"elapsed":34668,"user":{"displayName":"Kacper Zieniuk","userId":"05234677676847836894"}},"outputId":"2129f736-9d35-44ac-a2dc-87f49e2fabb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","Skipped perturbation batch 0 (already exists)\n","Skipped gradient batch 0 (already exists)\n","Skipped linear batch 0 (already exists)\n","Skipped importance batch 0 (already exists)\n","10\n","Skipped perturbation batch 10 (already exists)\n","Skipped gradient batch 10 (already exists)\n","Skipped linear batch 10 (already exists)\n","Skipped importance batch 10 (already exists)\n","20\n","Skipped perturbation batch 20 (already exists)\n","Skipped gradient batch 20 (already exists)\n","Skipped linear batch 20 (already exists)\n","Skipped importance batch 20 (already exists)\n","30\n","Skipped perturbation batch 30 (already exists)\n","Skipped gradient batch 30 (already exists)\n","Skipped linear batch 30 (already exists)\n","Skipped importance batch 30 (already exists)\n","40\n","Skipped perturbation batch 40 (already exists)\n","Skipped gradient batch 40 (already exists)\n","Skipped linear batch 40 (already exists)\n","Skipped importance batch 40 (already exists)\n","50\n","Skipped perturbation batch 50 (already exists)\n","Skipped gradient batch 50 (already exists)\n","Skipped linear batch 50 (already exists)\n","Skipped importance batch 50 (already exists)\n","60\n","Skipped perturbation batch 60 (already exists)\n","Skipped gradient batch 60 (already exists)\n","Skipped linear batch 60 (already exists)\n","Skipped importance batch 60 (already exists)\n","70\n","Skipped perturbation batch 70 (already exists)\n","Skipped gradient batch 70 (already exists)\n","Skipped linear batch 70 (already exists)\n","Skipped importance batch 70 (already exists)\n","80\n","Skipped perturbation batch 80 (already exists)\n","Skipped gradient batch 80 (already exists)\n","Skipped linear batch 80 (already exists)\n","Skipped importance batch 80 (already exists)\n","90\n","Skipped perturbation batch 90 (already exists)\n","Skipped gradient batch 90 (already exists)\n","Skipped linear batch 90 (already exists)\n","Skipped importance batch 90 (already exists)\n","100\n","Skipped perturbation batch 100 (already exists)\n","Skipped gradient batch 100 (already exists)\n","Skipped linear batch 100 (already exists)\n","Skipped importance batch 100 (already exists)\n","110\n","Skipped perturbation batch 110 (already exists)\n","Skipped gradient batch 110 (already exists)\n","Skipped linear batch 110 (already exists)\n","Skipped importance batch 110 (already exists)\n","120\n","Skipped perturbation batch 120 (already exists)\n","Skipped gradient batch 120 (already exists)\n","Skipped linear batch 120 (already exists)\n","Skipped importance batch 120 (already exists)\n","130\n","Skipped perturbation batch 130 (already exists)\n","Skipped gradient batch 130 (already exists)\n","Skipped linear batch 130 (already exists)\n","Skipped importance batch 130 (already exists)\n","140\n","Skipped perturbation batch 140 (already exists)\n","Skipped gradient batch 140 (already exists)\n","Skipped linear batch 140 (already exists)\n","Skipped importance batch 140 (already exists)\n","150\n","Skipped perturbation batch 150 (already exists)\n","Skipped gradient batch 150 (already exists)\n","Skipped linear batch 150 (already exists)\n","Skipped importance batch 150 (already exists)\n","160\n","Skipped perturbation batch 160 (already exists)\n","Skipped gradient batch 160 (already exists)\n","Skipped linear batch 160 (already exists)\n","Skipped importance batch 160 (already exists)\n","170\n","Skipped perturbation batch 170 (already exists)\n","Skipped gradient batch 170 (already exists)\n","Skipped linear batch 170 (already exists)\n","Skipped importance batch 170 (already exists)\n","180\n","Skipped perturbation batch 180 (already exists)\n","Skipped gradient batch 180 (already exists)\n","Skipped linear batch 180 (already exists)\n","Skipped importance batch 180 (already exists)\n","190\n","Skipped perturbation batch 190 (already exists)\n","Skipped gradient batch 190 (already exists)\n","Skipped linear batch 190 (already exists)\n","Skipped importance batch 190 (already exists)\n","200\n","Skipped perturbation batch 200 (already exists)\n","Skipped gradient batch 200 (already exists)\n","Skipped linear batch 200 (already exists)\n","Skipped importance batch 200 (already exists)\n","210\n","Skipped perturbation batch 210 (already exists)\n","Skipped gradient batch 210 (already exists)\n","Skipped linear batch 210 (already exists)\n","Skipped importance batch 210 (already exists)\n","220\n","Skipped perturbation batch 220 (already exists)\n","Skipped gradient batch 220 (already exists)\n","Skipped linear batch 220 (already exists)\n","Skipped importance batch 220 (already exists)\n","230\n","Skipped perturbation batch 230 (already exists)\n","Skipped gradient batch 230 (already exists)\n","Skipped linear batch 230 (already exists)\n","Skipped importance batch 230 (already exists)\n","240\n","Skipped perturbation batch 240 (already exists)\n","Skipped gradient batch 240 (already exists)\n","Skipped linear batch 240 (already exists)\n","Skipped importance batch 240 (already exists)\n","250\n","Skipped perturbation batch 250 (already exists)\n","Skipped gradient batch 250 (already exists)\n","Skipped linear batch 250 (already exists)\n","Skipped importance batch 250 (already exists)\n","260\n","Skipped perturbation batch 260 (already exists)\n","Skipped gradient batch 260 (already exists)\n","Skipped linear batch 260 (already exists)\n","Skipped importance batch 260 (already exists)\n","270\n","Skipped perturbation batch 270 (already exists)\n","Skipped gradient batch 270 (already exists)\n","Skipped linear batch 270 (already exists)\n","Skipped importance batch 270 (already exists)\n","280\n","Skipped perturbation batch 280 (already exists)\n","Skipped gradient batch 280 (already exists)\n","Skipped linear batch 280 (already exists)\n","Skipped importance batch 280 (already exists)\n","290\n","Skipped perturbation batch 290 (already exists)\n","Skipped gradient batch 290 (already exists)\n","Skipped linear batch 290 (already exists)\n","Skipped importance batch 290 (already exists)\n","300\n","Skipped perturbation batch 300 (already exists)\n","Skipped gradient batch 300 (already exists)\n","Skipped linear batch 300 (already exists)\n","Skipped importance batch 300 (already exists)\n","310\n","Skipped perturbation batch 310 (already exists)\n","Skipped gradient batch 310 (already exists)\n","Skipped linear batch 310 (already exists)\n","Skipped importance batch 310 (already exists)\n","320\n","Skipped perturbation batch 320 (already exists)\n","Skipped gradient batch 320 (already exists)\n","Skipped linear batch 320 (already exists)\n","Skipped importance batch 320 (already exists)\n","330\n","Skipped perturbation batch 330 (already exists)\n","Skipped gradient batch 330 (already exists)\n","Skipped linear batch 330 (already exists)\n","Skipped importance batch 330 (already exists)\n","340\n","Skipped perturbation batch 340 (already exists)\n","Skipped gradient batch 340 (already exists)\n","Skipped linear batch 340 (already exists)\n","Skipped importance batch 340 (already exists)\n","350\n","Skipped perturbation batch 350 (already exists)\n","Skipped gradient batch 350 (already exists)\n","Skipped linear batch 350 (already exists)\n","Skipped importance batch 350 (already exists)\n","360\n","Skipped perturbation batch 360 (already exists)\n","Skipped gradient batch 360 (already exists)\n","Skipped linear batch 360 (already exists)\n","Skipped importance batch 360 (already exists)\n","370\n","Skipped perturbation batch 370 (already exists)\n","Skipped gradient batch 370 (already exists)\n","Skipped linear batch 370 (already exists)\n","Skipped importance batch 370 (already exists)\n","380\n","Skipped perturbation batch 380 (already exists)\n","Skipped gradient batch 380 (already exists)\n","Skipped linear batch 380 (already exists)\n","Skipped importance batch 380 (already exists)\n","390\n","Skipped perturbation batch 390 (already exists)\n","Skipped gradient batch 390 (already exists)\n","Skipped linear batch 390 (already exists)\n","Skipped importance batch 390 (already exists)\n","400\n","Skipped perturbation batch 400 (already exists)\n","Skipped gradient batch 400 (already exists)\n","Skipped linear batch 400 (already exists)\n","Skipped importance batch 400 (already exists)\n","410\n","Skipped perturbation batch 410 (already exists)\n","Skipped gradient batch 410 (already exists)\n","Skipped linear batch 410 (already exists)\n","Skipped importance batch 410 (already exists)\n","420\n","Skipped perturbation batch 420 (already exists)\n","Skipped gradient batch 420 (already exists)\n","Skipped linear batch 420 (already exists)\n","Skipped importance batch 420 (already exists)\n","430\n","Skipped perturbation batch 430 (already exists)\n","Skipped gradient batch 430 (already exists)\n","Skipped linear batch 430 (already exists)\n","Skipped importance batch 430 (already exists)\n","440\n","Skipped perturbation batch 440 (already exists)\n","Skipped gradient batch 440 (already exists)\n","Skipped linear batch 440 (already exists)\n","Skipped importance batch 440 (already exists)\n","450\n","Skipped perturbation batch 450 (already exists)\n","Skipped gradient batch 450 (already exists)\n","Skipped linear batch 450 (already exists)\n","Skipped importance batch 450 (already exists)\n","460\n","Skipped perturbation batch 460 (already exists)\n","Skipped gradient batch 460 (already exists)\n","Skipped linear batch 460 (already exists)\n","Skipped importance batch 460 (already exists)\n","470\n","Skipped perturbation batch 470 (already exists)\n","Skipped gradient batch 470 (already exists)\n","Skipped linear batch 470 (already exists)\n","Skipped importance batch 470 (already exists)\n","480\n","Skipped perturbation batch 480 (already exists)\n","Skipped gradient batch 480 (already exists)\n","Skipped linear batch 480 (already exists)\n","Skipped importance batch 480 (already exists)\n","490\n","Skipped perturbation batch 490 (already exists)\n","Skipped gradient batch 490 (already exists)\n","Skipped linear batch 490 (already exists)\n","Skipped importance batch 490 (already exists)\n","500\n","Skipped perturbation batch 500 (already exists)\n","Skipped gradient batch 500 (already exists)\n","Skipped linear batch 500 (already exists)\n","Skipped importance batch 500 (already exists)\n","510\n","Skipped perturbation batch 510 (already exists)\n","Skipped gradient batch 510 (already exists)\n","Skipped linear batch 510 (already exists)\n","Skipped importance batch 510 (already exists)\n","520\n","Skipped perturbation batch 520 (already exists)\n","Skipped gradient batch 520 (already exists)\n","Skipped linear batch 520 (already exists)\n","Skipped importance batch 520 (already exists)\n","530\n","Skipped perturbation batch 530 (already exists)\n","Skipped gradient batch 530 (already exists)\n","Skipped linear batch 530 (already exists)\n","Skipped importance batch 530 (already exists)\n","540\n","Skipped perturbation batch 540 (already exists)\n","Skipped gradient batch 540 (already exists)\n","Skipped linear batch 540 (already exists)\n","Skipped importance batch 540 (already exists)\n","550\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _xla_gc_callback at 0x796826114cc0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","    \n","KeyboardInterrupt: \n"]},{"output_type":"stream","name":"stdout","text":["Skipped perturbation batch 550 (already exists)\n","Skipped gradient batch 550 (already exists)\n","Skipped linear batch 550 (already exists)\n","Skipped importance batch 550 (already exists)\n","560\n","Skipped perturbation batch 560 (already exists)\n","Skipped gradient batch 560 (already exists)\n","Skipped linear batch 560 (already exists)\n","Skipped importance batch 560 (already exists)\n","570\n","Skipped perturbation batch 570 (already exists)\n","Skipped gradient batch 570 (already exists)\n","Skipped linear batch 570 (already exists)\n","Skipped importance batch 570 (already exists)\n","580\n","Skipped perturbation batch 580 (already exists)\n","Skipped gradient batch 580 (already exists)\n","Skipped linear batch 580 (already exists)\n","Skipped importance batch 580 (already exists)\n","590\n","Skipped perturbation batch 590 (already exists)\n","Skipped gradient batch 590 (already exists)\n","Skipped linear batch 590 (already exists)\n","Skipped importance batch 590 (already exists)\n","600\n","Skipped perturbation batch 600 (already exists)\n","Skipped gradient batch 600 (already exists)\n","Skipped linear batch 600 (already exists)\n","Skipped importance batch 600 (already exists)\n","610\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _xla_gc_callback at 0x796826114cc0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","    \n","KeyboardInterrupt: \n"]},{"output_type":"stream","name":"stdout","text":["Skipped perturbation batch 610 (already exists)\n","Skipped gradient batch 610 (already exists)\n","Skipped linear batch 610 (already exists)\n","Skipped importance batch 610 (already exists)\n","620\n","Skipped perturbation batch 620 (already exists)\n","Skipped gradient batch 620 (already exists)\n","Skipped linear batch 620 (already exists)\n","Skipped importance batch 620 (already exists)\n","630\n","Skipped perturbation batch 630 (already exists)\n","Skipped gradient batch 630 (already exists)\n","Skipped linear batch 630 (already exists)\n","Skipped importance batch 630 (already exists)\n","640\n","Skipped perturbation batch 640 (already exists)\n","Skipped gradient batch 640 (already exists)\n","Skipped linear batch 640 (already exists)\n","Skipped importance batch 640 (already exists)\n","650\n","Skipped perturbation batch 650 (already exists)\n","Skipped gradient batch 650 (already exists)\n","Skipped linear batch 650 (already exists)\n","Skipped importance batch 650 (already exists)\n","660\n","Skipped perturbation batch 660 (already exists)\n","Skipped gradient batch 660 (already exists)\n","Skipped linear batch 660 (already exists)\n","Skipped importance batch 660 (already exists)\n","670\n","Skipped perturbation batch 670 (already exists)\n","Skipped gradient batch 670 (already exists)\n","Skipped linear batch 670 (already exists)\n","Skipped importance batch 670 (already exists)\n","680\n","Skipped perturbation batch 680 (already exists)\n","Skipped gradient batch 680 (already exists)\n","Skipped linear batch 680 (already exists)\n","Skipped importance batch 680 (already exists)\n","690\n","Skipped perturbation batch 690 (already exists)\n","Skipped gradient batch 690 (already exists)\n","Skipped linear batch 690 (already exists)\n","Skipped importance batch 690 (already exists)\n","700\n","Skipped perturbation batch 700 (already exists)\n","Skipped gradient batch 700 (already exists)\n","Skipped linear batch 700 (already exists)\n","Skipped importance batch 700 (already exists)\n","710\n","Skipped perturbation batch 710 (already exists)\n","Skipped gradient batch 710 (already exists)\n","Skipped linear batch 710 (already exists)\n","Skipped importance batch 710 (already exists)\n","720\n","Skipped perturbation batch 720 (already exists)\n","Skipped gradient batch 720 (already exists)\n","Skipped linear batch 720 (already exists)\n","Skipped importance batch 720 (already exists)\n","730\n","Skipped perturbation batch 730 (already exists)\n","Skipped gradient batch 730 (already exists)\n","Skipped linear batch 730 (already exists)\n","Skipped importance batch 730 (already exists)\n","740\n","Skipped perturbation batch 740 (already exists)\n","Skipped gradient batch 740 (already exists)\n","Skipped linear batch 740 (already exists)\n","Skipped importance batch 740 (already exists)\n","750\n","Skipped perturbation batch 750 (already exists)\n","Skipped gradient batch 750 (already exists)\n","Skipped linear batch 750 (already exists)\n","Skipped importance batch 750 (already exists)\n","760\n","Skipped perturbation batch 760 (already exists)\n","Skipped gradient batch 760 (already exists)\n","Skipped linear batch 760 (already exists)\n","Skipped importance batch 760 (already exists)\n","770\n","Skipped perturbation batch 770 (already exists)\n","Skipped gradient batch 770 (already exists)\n","Skipped linear batch 770 (already exists)\n","Skipped importance batch 770 (already exists)\n","780\n","Skipped perturbation batch 780 (already exists)\n","Skipped gradient batch 780 (already exists)\n","Skipped linear batch 780 (already exists)\n","Skipped importance batch 780 (already exists)\n","790\n","Skipped perturbation batch 790 (already exists)\n","Skipped gradient batch 790 (already exists)\n","Skipped linear batch 790 (already exists)\n","Skipped importance batch 790 (already exists)\n","800\n","Skipped perturbation batch 800 (already exists)\n","Skipped gradient batch 800 (already exists)\n","Skipped linear batch 800 (already exists)\n","Skipped importance batch 800 (already exists)\n","810\n","Skipped perturbation batch 810 (already exists)\n","Skipped gradient batch 810 (already exists)\n","Skipped linear batch 810 (already exists)\n","Skipped importance batch 810 (already exists)\n","820\n","Skipped perturbation batch 820 (already exists)\n","Skipped gradient batch 820 (already exists)\n","Skipped linear batch 820 (already exists)\n","Skipped importance batch 820 (already exists)\n","830\n","Skipped perturbation batch 830 (already exists)\n","Skipped gradient batch 830 (already exists)\n","Skipped linear batch 830 (already exists)\n","Skipped importance batch 830 (already exists)\n","840\n","Skipped perturbation batch 840 (already exists)\n","Skipped gradient batch 840 (already exists)\n","Skipped linear batch 840 (already exists)\n","Skipped importance batch 840 (already exists)\n","850\n","Skipped perturbation batch 850 (already exists)\n","Skipped gradient batch 850 (already exists)\n","Skipped linear batch 850 (already exists)\n","Skipped importance batch 850 (already exists)\n","860\n","Skipped perturbation batch 860 (already exists)\n","Skipped gradient batch 860 (already exists)\n","Skipped linear batch 860 (already exists)\n","Skipped importance batch 860 (already exists)\n","870\n","Skipped perturbation batch 870 (already exists)\n","Skipped gradient batch 870 (already exists)\n","Skipped linear batch 870 (already exists)\n","Skipped importance batch 870 (already exists)\n","880\n","Skipped perturbation batch 880 (already exists)\n","Skipped gradient batch 880 (already exists)\n","Skipped linear batch 880 (already exists)\n","Skipped importance batch 880 (already exists)\n","890\n","Skipped perturbation batch 890 (already exists)\n","Skipped gradient batch 890 (already exists)\n","Skipped linear batch 890 (already exists)\n","Skipped importance batch 890 (already exists)\n","900\n","Skipped perturbation batch 900 (already exists)\n","Skipped gradient batch 900 (already exists)\n","Skipped linear batch 900 (already exists)\n","Skipped importance batch 900 (already exists)\n","910\n","Skipped perturbation batch 910 (already exists)\n","Skipped gradient batch 910 (already exists)\n","Skipped linear batch 910 (already exists)\n","Skipped importance batch 910 (already exists)\n","920\n","Skipped perturbation batch 920 (already exists)\n","Skipped gradient batch 920 (already exists)\n","Skipped linear batch 920 (already exists)\n","Skipped importance batch 920 (already exists)\n","930\n","Skipped perturbation batch 930 (already exists)\n","Skipped gradient batch 930 (already exists)\n","Skipped linear batch 930 (already exists)\n","Skipped importance batch 930 (already exists)\n","940\n","Skipped perturbation batch 940 (already exists)\n","Skipped gradient batch 940 (already exists)\n","Skipped linear batch 940 (already exists)\n","Skipped importance batch 940 (already exists)\n","950\n","Skipped perturbation batch 950 (already exists)\n","Skipped gradient batch 950 (already exists)\n","Skipped linear batch 950 (already exists)\n","Skipped importance batch 950 (already exists)\n","960\n","Skipped perturbation batch 960 (already exists)\n","Skipped gradient batch 960 (already exists)\n","Skipped linear batch 960 (already exists)\n","Skipped importance batch 960 (already exists)\n","970\n","Skipped perturbation batch 970 (already exists)\n","Skipped gradient batch 970 (already exists)\n","Skipped linear batch 970 (already exists)\n","Skipped importance batch 970 (already exists)\n","980\n","Skipped perturbation batch 980 (already exists)\n","Skipped gradient batch 980 (already exists)\n","Skipped linear batch 980 (already exists)\n","Skipped importance batch 980 (already exists)\n","990\n","Skipped perturbation batch 990 (already exists)\n","Skipped gradient batch 990 (already exists)\n","Skipped linear batch 990 (already exists)\n","Skipped importance batch 990 (already exists)\n","1000\n","Skipped perturbation batch 1000 (already exists)\n","Skipped gradient batch 1000 (already exists)\n","Skipped linear batch 1000 (already exists)\n","Skipped importance batch 1000 (already exists)\n","1010\n","Skipped perturbation batch 1010 (already exists)\n","Skipped gradient batch 1010 (already exists)\n","Skipped linear batch 1010 (already exists)\n","Skipped importance batch 1010 (already exists)\n","1020\n","Skipped perturbation batch 1020 (already exists)\n","Skipped gradient batch 1020 (already exists)\n","Skipped linear batch 1020 (already exists)\n","Skipped importance batch 1020 (already exists)\n","1030\n","Skipped perturbation batch 1030 (already exists)\n","Skipped gradient batch 1030 (already exists)\n","Skipped linear batch 1030 (already exists)\n","Skipped importance batch 1030 (already exists)\n","1040\n","Skipped perturbation batch 1040 (already exists)\n","Skipped gradient batch 1040 (already exists)\n","Skipped linear batch 1040 (already exists)\n","Skipped importance batch 1040 (already exists)\n","1050\n","Skipped perturbation batch 1050 (already exists)\n","Skipped gradient batch 1050 (already exists)\n","Skipped linear batch 1050 (already exists)\n","Skipped importance batch 1050 (already exists)\n","1060\n","Skipped perturbation batch 1060 (already exists)\n","Skipped gradient batch 1060 (already exists)\n","Skipped linear batch 1060 (already exists)\n","Skipped importance batch 1060 (already exists)\n","1070\n","Skipped perturbation batch 1070 (already exists)\n","Skipped gradient batch 1070 (already exists)\n","Skipped linear batch 1070 (already exists)\n","Skipped importance batch 1070 (already exists)\n","1080\n","Skipped perturbation batch 1080 (already exists)\n","Skipped gradient batch 1080 (already exists)\n","Skipped linear batch 1080 (already exists)\n","Skipped importance batch 1080 (already exists)\n","1090\n","Skipped perturbation batch 1090 (already exists)\n","Skipped gradient batch 1090 (already exists)\n","Skipped linear batch 1090 (already exists)\n","Skipped importance batch 1090 (already exists)\n","1100\n","Skipped perturbation batch 1100 (already exists)\n","Skipped gradient batch 1100 (already exists)\n","Skipped linear batch 1100 (already exists)\n","Skipped importance batch 1100 (already exists)\n","1110\n","Skipped perturbation batch 1110 (already exists)\n","Skipped gradient batch 1110 (already exists)\n","Skipped linear batch 1110 (already exists)\n","Skipped importance batch 1110 (already exists)\n","1120\n","Skipped perturbation batch 1120 (already exists)\n","Skipped gradient batch 1120 (already exists)\n","Skipped linear batch 1120 (already exists)\n","Skipped importance batch 1120 (already exists)\n","1130\n","Skipped perturbation batch 1130 (already exists)\n","Skipped gradient batch 1130 (already exists)\n","Skipped linear batch 1130 (already exists)\n","Skipped importance batch 1130 (already exists)\n","1140\n","Skipped perturbation batch 1140 (already exists)\n","Skipped gradient batch 1140 (already exists)\n","Skipped linear batch 1140 (already exists)\n","Skipped importance batch 1140 (already exists)\n","1150\n","Skipped perturbation batch 1150 (already exists)\n","Skipped gradient batch 1150 (already exists)\n","Skipped linear batch 1150 (already exists)\n","Skipped importance batch 1150 (already exists)\n","1160\n","Skipped perturbation batch 1160 (already exists)\n","Skipped gradient batch 1160 (already exists)\n","Skipped linear batch 1160 (already exists)\n","Skipped importance batch 1160 (already exists)\n","1170\n","Skipped perturbation batch 1170 (already exists)\n","Skipped gradient batch 1170 (already exists)\n","Skipped linear batch 1170 (already exists)\n","Skipped importance batch 1170 (already exists)\n","1180\n","Skipped perturbation batch 1180 (already exists)\n","Skipped gradient batch 1180 (already exists)\n","Skipped linear batch 1180 (already exists)\n","Skipped importance batch 1180 (already exists)\n","1190\n","Skipped perturbation batch 1190 (already exists)\n","Skipped gradient batch 1190 (already exists)\n","Skipped linear batch 1190 (already exists)\n","Skipped importance batch 1190 (already exists)\n","1200\n","Skipped perturbation batch 1200 (already exists)\n","Skipped gradient batch 1200 (already exists)\n","Skipped linear batch 1200 (already exists)\n","Skipped importance batch 1200 (already exists)\n","1210\n","Skipped perturbation batch 1210 (already exists)\n","Skipped gradient batch 1210 (already exists)\n","Skipped linear batch 1210 (already exists)\n","Skipped importance batch 1210 (already exists)\n","1220\n","Skipped perturbation batch 1220 (already exists)\n","Skipped gradient batch 1220 (already exists)\n","Skipped linear batch 1220 (already exists)\n","Skipped importance batch 1220 (already exists)\n","1230\n","Skipped perturbation batch 1230 (already exists)\n","Skipped gradient batch 1230 (already exists)\n","Skipped linear batch 1230 (already exists)\n","Skipped importance batch 1230 (already exists)\n","1240\n","Skipped perturbation batch 1240 (already exists)\n","Skipped gradient batch 1240 (already exists)\n","Skipped linear batch 1240 (already exists)\n","Skipped importance batch 1240 (already exists)\n","1250\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _xla_gc_callback at 0x796826114cc0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","    \n","KeyboardInterrupt: \n"]},{"output_type":"stream","name":"stdout","text":["Skipped perturbation batch 1250 (already exists)\n","Skipped gradient batch 1250 (already exists)\n","Skipped linear batch 1250 (already exists)\n","Skipped importance batch 1250 (already exists)\n","1260\n","Skipped perturbation batch 1260 (already exists)\n","Skipped gradient batch 1260 (already exists)\n","Skipped linear batch 1260 (already exists)\n","Skipped importance batch 1260 (already exists)\n","1270\n","Skipped perturbation batch 1270 (already exists)\n","Skipped gradient batch 1270 (already exists)\n","Skipped linear batch 1270 (already exists)\n","Skipped importance batch 1270 (already exists)\n","1280\n","Skipped perturbation batch 1280 (already exists)\n","Skipped gradient batch 1280 (already exists)\n","Skipped linear batch 1280 (already exists)\n","Skipped importance batch 1280 (already exists)\n","1290\n","Skipped perturbation batch 1290 (already exists)\n","Skipped gradient batch 1290 (already exists)\n","Skipped linear batch 1290 (already exists)\n","Skipped importance batch 1290 (already exists)\n","1300\n","Skipped perturbation batch 1300 (already exists)\n","Skipped gradient batch 1300 (already exists)\n","Skipped linear batch 1300 (already exists)\n","Skipped importance batch 1300 (already exists)\n","1310\n","Skipped perturbation batch 1310 (already exists)\n","Skipped gradient batch 1310 (already exists)\n","Skipped linear batch 1310 (already exists)\n","Skipped importance batch 1310 (already exists)\n","1320\n","Skipped perturbation batch 1320 (already exists)\n","Skipped gradient batch 1320 (already exists)\n","Skipped linear batch 1320 (already exists)\n","Skipped importance batch 1320 (already exists)\n","1330\n","Skipped perturbation batch 1330 (already exists)\n","Skipped gradient batch 1330 (already exists)\n","Skipped linear batch 1330 (already exists)\n","Skipped importance batch 1330 (already exists)\n","1340\n","Skipped perturbation batch 1340 (already exists)\n","Skipped gradient batch 1340 (already exists)\n","Skipped linear batch 1340 (already exists)\n","Skipped importance batch 1340 (already exists)\n","1350\n","Skipped perturbation batch 1350 (already exists)\n","Skipped gradient batch 1350 (already exists)\n","Skipped linear batch 1350 (already exists)\n","Skipped importance batch 1350 (already exists)\n","1360\n","Skipped perturbation batch 1360 (already exists)\n","Skipped gradient batch 1360 (already exists)\n","Skipped linear batch 1360 (already exists)\n","Skipped importance batch 1360 (already exists)\n","1370\n","Skipped perturbation batch 1370 (already exists)\n","Skipped gradient batch 1370 (already exists)\n","Skipped linear batch 1370 (already exists)\n","Skipped importance batch 1370 (already exists)\n","1380\n","Skipped perturbation batch 1380 (already exists)\n","Skipped gradient batch 1380 (already exists)\n","Skipped linear batch 1380 (already exists)\n","Skipped importance batch 1380 (already exists)\n","1390\n","Skipped perturbation batch 1390 (already exists)\n","Skipped gradient batch 1390 (already exists)\n","Skipped linear batch 1390 (already exists)\n","Skipped importance batch 1390 (already exists)\n","1400\n","Skipped perturbation batch 1400 (already exists)\n","Skipped gradient batch 1400 (already exists)\n","Skipped linear batch 1400 (already exists)\n","Skipped importance batch 1400 (already exists)\n","1410\n","Skipped perturbation batch 1410 (already exists)\n","Skipped gradient batch 1410 (already exists)\n","Skipped linear batch 1410 (already exists)\n","Skipped importance batch 1410 (already exists)\n","1420\n","Skipped perturbation batch 1420 (already exists)\n","Skipped gradient batch 1420 (already exists)\n","Skipped linear batch 1420 (already exists)\n","Skipped importance batch 1420 (already exists)\n","1430\n","Skipped perturbation batch 1430 (already exists)\n","Skipped gradient batch 1430 (already exists)\n","Skipped linear batch 1430 (already exists)\n","Skipped importance batch 1430 (already exists)\n","1440\n","Skipped perturbation batch 1440 (already exists)\n","Skipped gradient batch 1440 (already exists)\n","Skipped linear batch 1440 (already exists)\n","Skipped importance batch 1440 (already exists)\n","1450\n","Skipped perturbation batch 1450 (already exists)\n","Skipped gradient batch 1450 (already exists)\n","Skipped linear batch 1450 (already exists)\n","Skipped importance batch 1450 (already exists)\n","1460\n","Skipped perturbation batch 1460 (already exists)\n","Skipped gradient batch 1460 (already exists)\n","Skipped linear batch 1460 (already exists)\n","Skipped importance batch 1460 (already exists)\n","1470\n","Skipped perturbation batch 1470 (already exists)\n","Skipped gradient batch 1470 (already exists)\n","Skipped linear batch 1470 (already exists)\n","Skipped importance batch 1470 (already exists)\n","1480\n","Skipped perturbation batch 1480 (already exists)\n","Skipped gradient batch 1480 (already exists)\n","Skipped linear batch 1480 (already exists)\n","Skipped importance batch 1480 (already exists)\n","1490\n","Skipped perturbation batch 1490 (already exists)\n","Skipped gradient batch 1490 (already exists)\n","Skipped linear batch 1490 (already exists)\n","Skipped importance batch 1490 (already exists)\n","1500\n","Skipped perturbation batch 1500 (already exists)\n","Skipped gradient batch 1500 (already exists)\n","Skipped linear batch 1500 (already exists)\n","Skipped importance batch 1500 (already exists)\n","1510\n","Skipped perturbation batch 1510 (already exists)\n","Skipped gradient batch 1510 (already exists)\n","Skipped linear batch 1510 (already exists)\n","Skipped importance batch 1510 (already exists)\n","1520\n","Skipped perturbation batch 1520 (already exists)\n","Skipped gradient batch 1520 (already exists)\n","Skipped linear batch 1520 (already exists)\n","Skipped importance batch 1520 (already exists)\n","1530\n","Skipped perturbation batch 1530 (already exists)\n","Skipped gradient batch 1530 (already exists)\n","Skipped linear batch 1530 (already exists)\n","Skipped importance batch 1530 (already exists)\n","1540\n","Skipped perturbation batch 1540 (already exists)\n","Skipped gradient batch 1540 (already exists)\n","Skipped linear batch 1540 (already exists)\n","Skipped importance batch 1540 (already exists)\n","1550\n","Skipped perturbation batch 1550 (already exists)\n","Skipped gradient batch 1550 (already exists)\n","Skipped linear batch 1550 (already exists)\n","Skipped importance batch 1550 (already exists)\n","1560\n","Skipped perturbation batch 1560 (already exists)\n","Skipped gradient batch 1560 (already exists)\n","Skipped linear batch 1560 (already exists)\n","Skipped importance batch 1560 (already exists)\n","1570\n","Skipped perturbation batch 1570 (already exists)\n","Skipped gradient batch 1570 (already exists)\n","Skipped linear batch 1570 (already exists)\n","Skipped importance batch 1570 (already exists)\n","1580\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-9-4148423150.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturbation_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mcalculate_mask_attribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperturbation_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Skipped perturbation batch {start_batch} (already exists)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4-3488300712.py\u001b[0m in \u001b[0;36mcalculate_mask_attribution\u001b[0;34m(texts, labels, k, save_path, debug)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mperturbed_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperturbed_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mperturbed_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturbed_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mattributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_conf\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mperturbed_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","model.eval()\n","model.to(device)\n","\n","if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","perturbation_folder = \"/content/drive/MyDrive/NLP_project/bert_eraser/results/perturbation\"\n","gradient_folder = \"/content/drive/MyDrive/NLP_project/bert_eraser/results/gradient\"\n","linear_folder = \"/content/drive/MyDrive/NLP_project/bert_eraser/results/linear\"\n","importance_folder = \"/content/drive/MyDrive/NLP_project/bert_eraser/results/importance\"\n","\n","\n","for start_batch in range(0, len(eraser_train), batch_size):\n","    print(start_batch)\n","    end_batch = start_batch + batch_size\n","    batch_texts = eraser_train['review'][start_batch:end_batch]\n","    batch_evidences = eraser_train['evidences'][start_batch:end_batch]\n","    batch_labels = eraser_train['label'][start_batch:end_batch]\n","    batch_labels = ['Positive' if l == 1 else 'Negative' for l in batch_labels]\n","\n","    perturbation_path = f\"{perturbation_folder}/batch_{start_batch}.parquet\"\n","    gradient_path = f\"{gradient_folder}/batch_{start_batch}.parquet\"\n","    linear_path = f\"{linear_folder}/batch_{start_batch}.parquet\"\n","    importance_path = f\"{importance_folder}/batch_{start_batch}.parquet\"\n","\n","    if not os.path.exists(perturbation_path):\n","        calculate_mask_attribution(batch_texts, batch_labels, save_path=perturbation_path)\n","    else:\n","        print(f\"Skipped perturbation batch {start_batch} (already exists)\")\n","\n","    if not os.path.exists(gradient_path):\n","        gradient_attribution_sentiment(batch_texts, batch_labels, save_path=gradient_path)\n","    else:\n","        print(f\"Skipped gradient batch {start_batch} (already exists)\")\n","\n","    if not os.path.exists(linear_path):\n","        linear_surrogate(batch_texts, batch_labels, save_path=linear_path)\n","    else:\n","        print(f\"Skipped linear batch {start_batch} (already exists)\")\n","\n","    if not os.path.exists(importance_path):\n","        evidence_string_dropout_sentiment(batch_texts, batch_evidences, batch_labels, save_path=importance_path)\n","    else:\n","        print(f\"Skipped importance batch {start_batch} (already exists)\")"]},{"cell_type":"code","source":["def _pos_prob(text, model, tokenizer, device):\n","    enc = tokenizer(\n","        text,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=True,\n","        max_length=512,\n","    ).to(device)\n","    with torch.no_grad():\n","        logits = model(**enc).logits\n","    return F.softmax(logits, dim=-1)[0, 1].item()\n","\n","def _drop_by_positions(text, positions, tokenizer):\n","    enc = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=512)\n","    offsets = enc[\"offset_mapping\"]\n","    keep_char = [True] * len(text)\n","\n","    for p in positions:\n","        idx = p + 1\n","        if idx < len(offsets):\n","            s, e = offsets[idx]\n","            for i in range(s, e):\n","                keep_char[i] = False\n","\n","    cleaned = \"\".join(ch for i, ch in enumerate(text) if keep_char[i])\n","    return re.sub(r\"\\s{2,}\", \" \", cleaned).strip()\n","\n","def _safe_list(obj):\n","    if isinstance(obj, list):\n","        return obj\n","    if isinstance(obj, str):\n","        try:\n","            return json.loads(obj)\n","        except json.JSONDecodeError:\n","            pass\n","    return []\n","\n","def _get_global_id(row, batch_num, local_idx):\n","    return int(getattr(row, \"id\", batch_num + local_idx))\n","\n","def evaluate_all_attributions(\n","    perturb_folder: str,\n","    gradient_folder: str,\n","    linear_folder: str,\n","    tokenizer,\n","    model,\n","    size: int = 1600,\n","    step: int = 10,\n","    k: int = 15,\n","    save_path: str = None,\n","):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device).eval()\n","\n","    folders = {\n","        \"perturb\":  perturb_folder,\n","        \"gradient\": gradient_folder,\n","        \"linear\":   linear_folder,\n","    }\n","\n","    out_rows = []\n","\n","    for batch_num in tqdm(range(0, size, step), desc=\"Batches\"):\n","        fname = f\"batch_{batch_num}.parquet\"\n","        dfs = {\n","            m: pd.read_parquet(os.path.join(path, fname))\n","            for m, path in folders.items()\n","            if os.path.exists(os.path.join(path, fname))\n","        }\n","\n","        for method, df in dfs.items():\n","            for local_idx, row in enumerate(df.itertuples(index=False)):\n","                text  = row.text\n","                label = getattr(row, \"label\", None)\n","\n","                pos  = _safe_list(row.token_positions)[:k]\n","                toks = _safe_list(row.tokens)[:k]\n","\n","                prob_full = _pos_prob(text, model, tokenizer, device)\n","\n","                dropped_txt = _drop_by_positions(text, pos, tokenizer)\n","                prob_drop   = _pos_prob(dropped_txt if dropped_txt else \".\", model, tokenizer, device)\n","\n","                ids = tokenizer.convert_tokens_to_ids(toks)\n","                ids = [\n","                    i if i is not None else tokenizer.unk_token_id\n","                    for i in ids\n","                ]\n","\n","                input_ids = []\n","                if tokenizer.cls_token_id is not None:\n","                    input_ids.append(tokenizer.cls_token_id)\n","\n","                input_ids.extend(ids)\n","\n","                end_tok = (\n","                    tokenizer.sep_token_id\n","                    if tokenizer.sep_token_id is not None\n","                    else tokenizer.eos_token_id\n","                )\n","                if end_tok is not None:\n","                    input_ids.append(end_tok)\n","\n","                mini_text   = tokenizer.decode(input_ids) if input_ids else \".\"\n","                prob_tokens = _pos_prob(mini_text, model, tokenizer, device)\n","\n","                out_rows.append(\n","                    dict(\n","                        global_id   = _get_global_id(row, batch_num, local_idx),\n","                        batch       = batch_num,\n","                        method      = method,\n","                        prob_full   = round(prob_full, 4),\n","                        prob_drop   = round(prob_drop, 4),\n","                        delta       = round(prob_full - prob_drop, 4),\n","                        prob_tokens = round(prob_tokens, 4),\n","                        label       = label,\n","                    )\n","                )\n","\n","    out_df = pd.DataFrame(out_rows)\n","    out_df.to_parquet(save_path, index=False)\n","    print(f\"Saved {len(out_df)} rows → {save_path}\")\n","    return out_df"],"metadata":{"id":"5amg55N0r2rE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["f4222a723a0245fb9d1d79e49de05cd2","dfe6231e9f694e9a965f8acf29a8ddf0","928eda0cb20441638119190b3725d420","d22bd2f19b8c40d4b7518edd66d077e5","4c899ca79efd4018abf2d0be16979180","4bf5a4bf30d3452692a8673dd49f91b0","fb7254143a6b4282b95d1baa1b7b7820","5c5af15c928643749e9615532e3c94f9","433b3bc216ff4c9f969077d8bb63181b","758db585e92c49739dc4dbf7d361e362","0b25058f0e6d4554a275a75ed4fd8172","c15a2cd276b64f299e8e5c05647754b3","e437f7eb18bd4a32b47a6abc4a158fd8","7c1bf260a1cf47989420cf59468bbf32","018752d5eb3643b3a5284f3b8eee7fc5","204bb55e2961444b8b937252582aff4b","9f3609b6514f4794b7be3925034c2604","661d5e91ba03464d813ee6cb259f6836","8490b088c74b41cd81c470783de8f1d2","ee6297ecd3974fe5b2d197c3ac6ea400","25ee3751318749ab84c5183dac1258c3","dfc0b29da91e4ab594b59e21c3438f1a","5503bdc5033c4796bf76094ddeb8e796","211f20ec2acb48848aedbbe2e66b536b","b556e60836c94ad09e982e8c7547627b","9a25f7e6339a4705badbc541eab294e6","a089e06b597345caada5ce241c230db2","9fd3569173554ae0ad75212568a6dc65","47fd3b79e9b446b2a5b37c4e03a29201","e75712a142fe4acdb22e15d517e1edbd","5d42b25656444123ba5538cdab5d291e","da84238ea00d44bb9a7fcfc13b4bae54","9d57b126f2c848719f3290ad1ebe6756","e5025bf2e1f647c4acc82c5ce8f187dd","5b2af17635524c1aa51209e56754d2b0","abdf40f8ec974dac94c3a877927f1e77","7eb783fd2b464845a98172c5a456721a","63e9e8e66b8d424a8a9508753999feab","9c33d3630b7c404e8ad6296dbe2e30cf","6df4e81a46a2404a86240a7b9f1de508","d3db7654991f4cecbcaeb41f713a2f99","d47001e2e7b840a980b63e2c9273b0ac","ad1d21e723474cda852a607d6ceaefd0","2f4d6e7450d94c5d8a08339c7f696e8b","6183ad6fd5b64850a3b3a6684943f89f","88d98f0994424f05acc1e6dc8ffc98f2","fe40e9eeec7344c5a8268a741eda6191","de66e41fd74d470db108afeeb964c4ab","8676f8d1f985442fa6a6dd3f09c9d756","5ec72664556348c4b2b2b3677d75ed75","291dad279218418d9f4f412349709f8b","a295f35378bc4c0493e8205ea43baf1f","73bb9e01e4f44d4da116d61436275e6e","7ba604d541164f70af7d0a880e2c833c","cb33ce87224545e7ba54644cf5dfe0d5"]},"id":"1kmQTMrFzR1E","collapsed":true,"outputId":"d18a6607-9b93-4301-84e9-cf33dab036da"},"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/748 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4222a723a0245fb9d1d79e49de05cd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c15a2cd276b64f299e8e5c05647754b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5503bdc5033c4796bf76094ddeb8e796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5025bf2e1f647c4acc82c5ce8f187dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6183ad6fd5b64850a3b3a6684943f89f"}},"metadata":{}}],"source":["model_name = \"PavanNeerudu/gpt2-finetuned-sst2\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","model.eval()\n","model.to(device)\n","\n","if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","perturbation_folder = \"/content/drive/MyDrive/NLP_project/gpt_eraser/results/perturbation\"\n","gradient_folder = \"/content/drive/MyDrive/NLP_project/gpt_eraser/results/gradient\"\n","linear_folder = \"/content/drive/MyDrive/NLP_project/gpt_eraser/results/linear\"\n","importance_folder = \"/content/drive/MyDrive/NLP_project/gpt_eraser/results/importance\"\n","\n","\n","for start_batch in range(0, len(eraser_train), batch_size):\n","    print(start_batch)\n","    end_batch = start_batch + batch_size\n","    batch_texts = eraser_train['review'][start_batch:end_batch]\n","    batch_evidences = eraser_train['evidences'][start_batch:end_batch]\n","    batch_labels = eraser_train['label'][start_batch:end_batch]\n","    batch_labels = ['Positive' if l == 1 else 'Negative' for l in batch_labels]\n","\n","    perturbation_path = f\"{perturbation_folder}/batch_{start_batch}.parquet\"\n","    gradient_path = f\"{gradient_folder}/batch_{start_batch}.parquet\"\n","    linear_path = f\"{linear_folder}/batch_{start_batch}.parquet\"\n","    importance_path = f\"{importance_folder}/batch_{start_batch}.parquet\"\n","\n","    if not os.path.exists(perturbation_path):\n","        calculate_mask_attribution(batch_texts, batch_labels, save_path=perturbation_path)\n","    else:\n","        print(f\"Skipped perturbation batch {start_batch} (already exists)\")\n","\n","    if not os.path.exists(gradient_path):\n","        gradient_attribution_sentiment(batch_texts, batch_labels, save_path=gradient_path)\n","    else:\n","        print(f\"Skipped gradient batch {start_batch} (already exists)\")\n","\n","    if not os.path.exists(linear_path):\n","        linear_surrogate(batch_texts, batch_labels, save_path=linear_path)\n","    else:\n","        print(f\"Skipped linear batch {start_batch} (already exists)\")\n","\n","    if not os.path.exists(importance_path):\n","        evidence_string_dropout_sentiment(batch_texts, batch_evidences, batch_labels, save_path=importance_path)\n","    else:\n","        print(f\"Skipped importance batch {start_batch} (already exists)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCrPgHAVAZ1a","colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"status":"error","timestamp":1751182706282,"user_tz":-120,"elapsed":2067,"user":{"displayName":"Kacper Zieniuk","userId":"05234677676847836894"}},"outputId":"d521eef7-571b-4291-ba5a-d38843411ecb"},"outputs":[{"output_type":"stream","name":"stderr","text":["emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"]},{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-34-1754932388.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportance_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mevidence_string_dropout_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_evidences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimportance_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Skipped importance batch {start_batch} (already exists)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-20-1633100940.py\u001b[0m in \u001b[0;36mevidence_string_dropout_sentiment\u001b[0;34m(texts, evidences, labels, debug, save_path, fname)\u001b[0m\n\u001b[1;32m     34\u001b[0m             ).to(device)\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mprob_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvariant\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1203\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 )\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[0m\u001b[1;32m    836\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["# # model_name = \"jphme/llama2-7b-sst2\"\n","# model_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\n","# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","# model.eval()\n","# model.to(device)\n","\n","# if tokenizer.pad_token is None:\n","#         tokenizer.pad_token = tokenizer.eos_token\n","\n","# perturbation_folder = \"/content/drive/MyDrive/NLP_project/llama_eraser/results/perturbation\"\n","# gradient_folder = \"/content/drive/MyDrive/NLP_project/llama_eraser/results/gradient\"\n","# linear_folder = \"/content/drive/MyDrive/NLP_project/llama_eraser/results/linear\"\n","# importance_folder = \"/content/drive/MyDrive/NLP_project/llama_eraser/results/importance\"\n","\n","\n","# for start_batch in range(0, 500, batch_size):\n","#     print(start_batch)\n","#     end_batch = start_batch + batch_size\n","#     batch_texts = eraser_train['review'][start_batch:end_batch]\n","#     batch_evidences = eraser_train['evidences'][start_batch:end_batch]\n","#     batch_labels = eraser_train['label'][start_batch:end_batch]\n","#     batch_labels = ['Positive' if l == 1 else 'Negative' for l in batch_labels]\n","\n","#     perturbation_path = f\"{perturbation_folder}/batch_{start_batch}.parquet\"\n","#     gradient_path = f\"{gradient_folder}/batch_{start_batch}.parquet\"\n","#     linear_path = f\"{linear_folder}/batch_{start_batch}.parquet\"\n","#     importance_path = f\"{importance_folder}/batch_{start_batch}.parquet\"\n","\n","#     # if not os.path.exists(perturbation_path):\n","#     #     calculate_mask_attribution(batch_texts, batch_labels, save_path=perturbation_path)\n","#     # else:\n","#     #     print(f\"Skipped perturbation batch {start_batch} (already exists)\")\n","\n","#     # if not os.path.exists(gradient_path):\n","#     #     gradient_attribution_sentiment(batch_texts, batch_labels, save_path=gradient_path)\n","#     # else:\n","#     #     print(f\"Skipped gradient batch {start_batch} (already exists)\")\n","\n","#     # if not os.path.exists(linear_path):\n","#     #     linear_surrogate(batch_texts, batch_labels, save_path=linear_path)\n","#     # else:\n","#     #     print(f\"Skipped linear batch {start_batch} (already exists)\")\n","\n","#     if not os.path.exists(importance_path):\n","#         evidence_string_dropout_sentiment(batch_texts, batch_evidences, batch_labels, save_path=importance_path)\n","#     else:\n","#         print(f\"Skipped importance batch {start_batch} (already exists)\")"]},{"cell_type":"code","source":["for model, folder_name in [(\"distilbert-base-uncased-finetuned-sst-2-english\", \"bert_eraser\"), (\"PavanNeerudu/gpt2-finetuned-sst2\", \"gpt_eraser\")]: #\"jphme/llama2-7b-sst2\"\n","    tokenizer  = AutoTokenizer.from_pretrained(model_name)\n","    for k in [5, 15]:\n","        mdl  = AutoModelForSequenceClassification.from_pretrained(model_name)\n","        perturbation_folder = f\"/content/drive/MyDrive/NLP_project/{folder_name}/results/perturbation\"\n","        gradient_folder = f\"/content/drive/MyDrive/NLP_project/{folder_name}/results/gradient\"\n","        linear_folder = f\"/content/drive/MyDrive/NLP_project/{folder_name}/results/linear\"\n","        eval_df = evaluate_all_attributions(\n","            perturb_folder=perturbation_folder,\n","            gradient_folder=gradient_folder,\n","            linear_folder=linear_folder,\n","            tokenizer=tokenizer,\n","            model=model,\n","            size=len(eraser_train),\n","            k=k,\n","            save_path=f'/content/drive/MyDrive/NLP_project/{folder_name}/results/comprehensiveness_{k}.parquet'\n","        )"],"metadata":{"id":"T7raWGtPqBhy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3F1nJfh2szW_"},"source":["## Multiple Choice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVxxQ3FBsyK_"},"outputs":[],"source":["cose = load_dataset(\"cos_e\", \"v1.11\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOQfYgF7t0VX"},"outputs":[],"source":["def calculate_mask_attribution_mcqa(question, choices, correct_idx, model, tokenizer, k=15, debug=False):\n","    import torch\n","    import numpy as np\n","\n","    device = next(model.parameters()).device\n","    model.eval()\n","\n","    original_texts = [f\"Premise: {question} Hypothesis: The answer is {c}\" for c in choices]\n","    inputs = tokenizer(original_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n","\n","    with torch.no_grad():\n","        logits = model(**inputs).logits\n","        entailment_scores = torch.softmax(logits, dim=-1)[:, 2]  # i\n","        original_probs = torch.softmax(entailment_scores, dim=-1)\n","        correct_prob = original_probs[correct_idx].item()\n","\n","    correct_input = original_texts[correct_idx]\n","    encoded = tokenizer(\n","        correct_input,\n","        return_offsets_mapping=True,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=True,\n","        max_length=512\n","    )\n","    offset_map = encoded[\"offset_mapping\"][0].tolist()\n","    token_ids = encoded[\"input_ids\"]\n","    tokens = tokenizer.convert_ids_to_tokens(token_ids[0])\n","\n","    static_prefixes = {\"Premise\", \"Hypothesis\", \"The\", \"answer\", \"is\", \":\"}\n","    valid_token_info = [\n","        (i, tok) for i, (tok, (start, end)) in enumerate(zip(tokens, offset_map))\n","        if start != end and tok not in static_prefixes and not tok.startswith(\"▁\") and start < len(question)\n","    ]\n","\n","    attributions = []\n","    for i, _ in valid_token_info:\n","        perturbed_texts = [\n","            f\"Premise: {question} Hypothesis: The answer is {c}\"\n","            for c in choices\n","        ]\n","        batch_encoding = tokenizer(\n","            perturbed_texts,\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            padding=True,\n","            max_length=512\n","        )\n","\n","        pert_input_ids = batch_encoding[\"input_ids\"]\n","        for idx in range(pert_input_ids.size(0)):\n","            if i < pert_input_ids.size(1):\n","                pert_input_ids[idx, i] = tokenizer.mask_token_id if tokenizer.mask_token_id else tokenizer.unk_token_id\n","\n","        pert_input_ids = pert_input_ids.to(device)\n","        pert_attention = (pert_input_ids != tokenizer.pad_token_id).long().to(device)\n","\n","        with torch.no_grad():\n","            pert_logits = model(input_ids=pert_input_ids, attention_mask=pert_attention).logits\n","            pert_entail_scores = torch.softmax(pert_logits, dim=-1)[:, 2]\n","            pert_probs = torch.softmax(pert_entail_scores, dim=-1)\n","\n","        diff = correct_prob - pert_probs[correct_idx].item()\n","        attributions.append(diff)\n","\n","    tokens_to_return = [tok for _, tok in valid_token_info]\n","    top_k = min(k, len(attributions))\n","    sorted_indices = sorted(range(len(attributions)), key=lambda j: abs(attributions[j]), reverse=True)[:top_k]\n","    top_tokens_scores = [(tokens_to_return[i], round(attributions[i], 4)) for i in sorted_indices]\n","    top_positions = [valid_token_info[i][0] for i in sorted_indices]\n","\n","    if debug:\n","        print(f\"\\nQ: {question}\")\n","        print(f\"Correct Answer: {choices[correct_idx]}\")\n","        print(\"Top influential question tokens:\")\n","        for tok, score in top_tokens_scores:\n","            print(f\"{tok:15} | weight: {score:+.5f}\")\n","        print(\"-\" * 80)\n","\n","    return {\n","        \"question\": question,\n","        \"correct_answer\": choices[correct_idx],\n","        \"tokens\": [t for t, _ in top_tokens_scores],\n","        \"weights\": [s for _, s in top_tokens_scores],\n","        \"token_positions\": top_positions\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HOiCYFxE56a3"},"outputs":[],"source":["def gradient_attribution_mcqa(question, choices, correct_idx, model, tokenizer, k=15, debug=False):\n","    import torch\n","\n","    device = next(model.parameters()).device\n","    model.eval()\n","\n","    input_text = f\"Premise: {question} Hypothesis: The answer is {choices[correct_idx]}\"\n","    encoded = tokenizer(\n","        input_text,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=True,\n","        max_length=512,\n","        return_offsets_mapping=True\n","    ).to(device)\n","\n","    input_ids = encoded[\"input_ids\"]\n","    attention_mask = encoded[\"attention_mask\"]\n","    offset_mapping = encoded[\"offset_mapping\"][0].tolist()\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n","\n","    if hasattr(model, \"distilbert\"):\n","        embedding_layer = model.distilbert.embeddings.word_embeddings\n","    elif hasattr(model, \"roberta\"):\n","        embedding_layer = model.roberta.embeddings.word_embeddings\n","    elif hasattr(model, \"model\") and hasattr(model.model, \"encoder\") and hasattr(model.model.encoder, \"embed_tokens\"):\n","        embedding_layer = model.model.encoder.embed_tokens\n","    else:\n","        raise ValueError(\"Model architecture not supported for gradient attribution.\")\n","\n","    inputs_embeds = embedding_layer(input_ids).detach().requires_grad_(True)\n","    outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n","    logits = outputs.logits\n","    target_logit = logits[0, 2]\n","    target_logit.backward()\n","\n","    grads = inputs_embeds.grad.abs().sum(dim=-1).squeeze()\n","\n","    question_start = input_text.index(question)\n","    question_end = question_start + len(question)\n","\n","    question_tokens_info = [\n","        (i, tok, grads[i].item())\n","        for i, (tok, (start, end)) in enumerate(zip(tokens, offset_mapping))\n","        if start >= question_start and end <= question_end and end > start\n","    ]\n","\n","    top_k = min(k, len(question_tokens_info))\n","    sorted_by_score = sorted(question_tokens_info, key=lambda x: abs(x[2]), reverse=True)[:top_k]\n","    top_tokens_scores = [(tok, round(score, 4)) for _, tok, score in sorted_by_score]\n","    token_positions = [i for i, _, _ in sorted_by_score]\n","\n","    if debug:\n","        print(f\"\\nQ: {question}\")\n","        print(f\"Correct Answer: {choices[correct_idx]}\")\n","        print(\"Top influential question tokens (gradient):\")\n","        for tok, score in top_tokens_scores:\n","            print(f\"{tok:15} | weight: {score:+.4f}\")\n","        print(\"-\" * 80)\n","\n","    return {\n","        \"question\": question,\n","        \"correct_answer\": choices[correct_idx],\n","        \"tokens\": [t for t, _ in top_tokens_scores],\n","        \"weights\": [s for _, s in top_tokens_scores],\n","        \"token_positions\": token_positions\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"O2Yo2qCX1SUL"},"outputs":[],"source":["# nli_model_name = \"roberta-large-mnli\"\n","# tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n","# model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).eval().to(device)\n","\n","\n","# example = cose['train'][2]\n","# question = example[\"question\"]\n","# choices = example[\"choices\"]\n","# correct_idx = choices.index(example[\"answer\"])\n","# correct = choices.index(example[\"answer\"])\n","\n","# res = calculate_mask_attribution_mcqa(\n","#     question, choices, correct_idx=correct,\n","#     model=model, tokenizer=tokenizer,\n","# )\n","# gradient_attribution_mcqa(\n","#     question=question,\n","#     choices=choices,\n","#     correct_idx=correct_idx,\n","#     model=model,\n","#     tokenizer=tokenizer,\n","#     debug=True\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGfc49R-7Uja"},"outputs":[],"source":["model_name = \"roberta-large-mnli\"\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","model.eval()\n","model.to(device)\n","\n","if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","perturbation_folder = \"/content/drive/MyDrive/NLP_project/bert_cos/results/perturbation\"\n","gradient_folder = \"/content/drive/MyDrive/NLP_project/bert_cos/results/gradient\"\n","\n","\n","for start_batch in range(0, 500, batch_size):\n","    print(start_batch)\n","\n","    results_perm = []\n","    results_gradient = []\n","    results_linear = []\n","\n","    perturbation_path = f\"{perturbation_folder}/batch_{start_batch}.parquet\"\n","    gradient_path = f\"{gradient_folder}/batch_{start_batch}.parquet\"\n","    if os.path.exists(perturbation_path):\n","        print(f\"Skipped perturbation batch {start_batch} (already exists)\")\n","        continue\n","\n","    for i in range(start_batch, start_batch + batch_size):\n","        example = cose['train'][i]\n","        question = example[\"question\"]\n","        choices = example[\"choices\"]\n","        correct_idx = choices.index(example[\"answer\"])\n","        results_perm.append(calculate_mask_attribution_mcqa(question, choices, correct_idx, model, tokenizer))\n","        results_gradient.append(gradient_attribution_mcqa(question, choices, correct_idx, model, tokenizer))\n","\n","    pd.DataFrame(results_perm).to_parquet(perturbation_path)\n","    pd.DataFrame(results_gradient).to_parquet(gradient_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0-0jX-38qBN"},"outputs":[],"source":["model_name = \"microsoft/deberta-v3-large-mnli\"\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","model.eval()\n","model.to(device)\n","\n","if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","perturbation_folder = \"/content/drive/MyDrive/NLP_project/deberta_cos/results/perturbation\"\n","gradient_folder = \"/content/drive/MyDrive/NLP_project/deberta_cos/results/gradient\"\n","\n","\n","for start_batch in range(0, 500, batch_size):\n","    print(start_batch)\n","\n","    results_perm = []\n","    results_gradient = []\n","    results_linear = []\n","\n","    perturbation_path = f\"{perturbation_folder}/batch_{start_batch}.parquet\"\n","    gradient_path = f\"{gradient_folder}/batch_{start_batch}.parquet\"\n","    if os.path.exists(perturbation_path):\n","        print(f\"Skipped perturbation batch {start_batch} (already exists)\")\n","        continue\n","\n","    for i in range(start_batch, start_batch + batch_size):\n","        example = cose['train'][i]\n","        question = example[\"question\"]\n","        choices = example[\"choices\"]\n","        correct_idx = choices.index(example[\"answer\"])\n","        results_perm.append(calculate_mask_attribution_mcqa(question, choices, correct_idx, model, tokenizer))\n","        results_gradient.append(gradient_attribution_mcqa(question, choices, correct_idx, model, tokenizer))\n","\n","    pd.DataFrame(results_perm).to_parquet(perturbation_path)\n","    pd.DataFrame(results_gradient).to_parquet(gradient_path)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"collapsed_sections":["3F1nJfh2szW_"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"10bd61e09a5a4a96af3e59a338caaffa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e84e8428a566438b8deaa1215446dd44","IPY_MODEL_cb612e76c64e4d9c9bce44fda350e766","IPY_MODEL_c9a05e1028d1461ca8067576bf305f23"],"layout":"IPY_MODEL_869d06ff455c47d2a3b2f4129a357312"}},"e84e8428a566438b8deaa1215446dd44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2abdbc2c05ae47cd883fb03d0616eedd","placeholder":"​","style":"IPY_MODEL_040d631ad49e42a08aea956d759356b6","value":"README.md: "}},"cb612e76c64e4d9c9bce44fda350e766":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a102703903f445a7838791daf452adaa","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_787cb2d18e0548109685f63c0a6fd50c","value":1}},"c9a05e1028d1461ca8067576bf305f23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57e46e2641814397af20d41d32cfe41f","placeholder":"​","style":"IPY_MODEL_e30b4888a73f4d9aaf5e9a2935a68fd8","value":" 6.65k/? [00:00&lt;00:00, 753kB/s]"}},"869d06ff455c47d2a3b2f4129a357312":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2abdbc2c05ae47cd883fb03d0616eedd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"040d631ad49e42a08aea956d759356b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a102703903f445a7838791daf452adaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"787cb2d18e0548109685f63c0a6fd50c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57e46e2641814397af20d41d32cfe41f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e30b4888a73f4d9aaf5e9a2935a68fd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd58f3b4d88f48ee92f65fa385a97d78":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c08792c5b5c4b87941b28894c0cc3f3","IPY_MODEL_2b4e2e9ca11443e5a4f96d64f4d16b95","IPY_MODEL_789ecf0c14e8462cabcc86a688b6d0b2"],"layout":"IPY_MODEL_01083b8ee6de4606afde6784444b35fe"}},"6c08792c5b5c4b87941b28894c0cc3f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69a0fdfb1164450ba3f181ce555a8fa4","placeholder":"​","style":"IPY_MODEL_cb4e16a79d6e4beca0d8da85a46a774e","value":"movie_rationales.py: "}},"2b4e2e9ca11443e5a4f96d64f4d16b95":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47b2489e7e074f5698f1653ecc77fe3c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8482c31c8d64c0c980476fa94c437cd","value":1}},"789ecf0c14e8462cabcc86a688b6d0b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec64838d039a4d12bebe287d49e48f8f","placeholder":"​","style":"IPY_MODEL_bf71a2b01c7c44888af641aa71d385ab","value":" 4.42k/? [00:00&lt;00:00, 423kB/s]"}},"01083b8ee6de4606afde6784444b35fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69a0fdfb1164450ba3f181ce555a8fa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb4e16a79d6e4beca0d8da85a46a774e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47b2489e7e074f5698f1653ecc77fe3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a8482c31c8d64c0c980476fa94c437cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec64838d039a4d12bebe287d49e48f8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf71a2b01c7c44888af641aa71d385ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da6a5fbf429045309dda967606a11a0b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c33a54a4863a431a90412dde9528d5f1","IPY_MODEL_89303d9a5d0a41a8bdfc28e6183b38f7","IPY_MODEL_1c2464d0e6e24aeca412172be61b5f32"],"layout":"IPY_MODEL_8498e3178f1c44619cd614010e14c767"}},"c33a54a4863a431a90412dde9528d5f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f963062566d4c39b04269676b729134","placeholder":"​","style":"IPY_MODEL_aa72d82768b246869f57d194f0b7531a","value":"Downloading data: 100%"}},"89303d9a5d0a41a8bdfc28e6183b38f7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ee0ab4deb5e4479bb8f2b9bbaeb10a2","max":3899487,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b35045127f784705bb9ebeaa077af4a1","value":3899487}},"1c2464d0e6e24aeca412172be61b5f32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_748abe5d4e7145c1a37ef9f6dbaa7001","placeholder":"​","style":"IPY_MODEL_49ca9a40a79d42569e75efc8b437ac6d","value":" 3.90M/3.90M [00:00&lt;00:00, 4.49MB/s]"}},"8498e3178f1c44619cd614010e14c767":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f963062566d4c39b04269676b729134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa72d82768b246869f57d194f0b7531a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ee0ab4deb5e4479bb8f2b9bbaeb10a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b35045127f784705bb9ebeaa077af4a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"748abe5d4e7145c1a37ef9f6dbaa7001":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49ca9a40a79d42569e75efc8b437ac6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6258faae35b9460391b29be4a59505be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_566229e310aa404cb58106d07fd353b0","IPY_MODEL_e7d3016a8bb64c7f905938e0d09b8aad","IPY_MODEL_0ab851ea3c124a4a8b6884d8fb2ec6e5"],"layout":"IPY_MODEL_308b1821bcb542bf8a8e446549b16375"}},"566229e310aa404cb58106d07fd353b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9f477ccfc5942b295d4ed3556a5cd7e","placeholder":"​","style":"IPY_MODEL_6b70c6ee8a8742f1bd2639590a93aa31","value":"Generating train split: 100%"}},"e7d3016a8bb64c7f905938e0d09b8aad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4989deeda2443e3881a8cd97804dd0d","max":1600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8841b4c11d0d40839959610d6f672571","value":1600}},"0ab851ea3c124a4a8b6884d8fb2ec6e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5f77d8c7b0d4340b706e68b75ab3544","placeholder":"​","style":"IPY_MODEL_441ef4548f1d4438b48e3dbeace79b08","value":" 1600/1600 [00:00&lt;00:00, 7530.04 examples/s]"}},"308b1821bcb542bf8a8e446549b16375":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9f477ccfc5942b295d4ed3556a5cd7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b70c6ee8a8742f1bd2639590a93aa31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4989deeda2443e3881a8cd97804dd0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8841b4c11d0d40839959610d6f672571":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5f77d8c7b0d4340b706e68b75ab3544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"441ef4548f1d4438b48e3dbeace79b08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ea3fbde57c949f2923397efece5679c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4063eb61c054109965414ecd9d076fd","IPY_MODEL_ed68c75460f345cd8b6c4e9c2e762a1f","IPY_MODEL_1b25ade8a1df40458486ed1a45b10e3c"],"layout":"IPY_MODEL_9de143c9441441b9940832606ef1b015"}},"a4063eb61c054109965414ecd9d076fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b374460a48749839a3e4f1724ad6906","placeholder":"​","style":"IPY_MODEL_9efaadf8fa0243feb085eae9d6a2d561","value":"Generating validation split: 100%"}},"ed68c75460f345cd8b6c4e9c2e762a1f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_33a8ac4de7874b92a3b582ab5efdad1c","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78ff3342e7094779b1b27b654ff5c899","value":200}},"1b25ade8a1df40458486ed1a45b10e3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3adc412200a416f8c33c613751691d8","placeholder":"​","style":"IPY_MODEL_56d132bb75ab43be8d8c1b2e1b501ad5","value":" 200/200 [00:00&lt;00:00,  7.26 examples/s]"}},"9de143c9441441b9940832606ef1b015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b374460a48749839a3e4f1724ad6906":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9efaadf8fa0243feb085eae9d6a2d561":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33a8ac4de7874b92a3b582ab5efdad1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78ff3342e7094779b1b27b654ff5c899":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3adc412200a416f8c33c613751691d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56d132bb75ab43be8d8c1b2e1b501ad5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac6b8264005a492d8a9c2783458794b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6e6ccd97244481ab01859650cfa8c41","IPY_MODEL_55378599df2a4f1fab43b5d4ccf25091","IPY_MODEL_af191d3b2b5340c7988d4d4646c5f87e"],"layout":"IPY_MODEL_ac1da6ac3a3e425692cc74092e2b8f04"}},"a6e6ccd97244481ab01859650cfa8c41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63f8d7b4a118482e92c4c050073c0883","placeholder":"​","style":"IPY_MODEL_e21b8473ed8943c5bd307d1381738845","value":"Generating test split: 100%"}},"55378599df2a4f1fab43b5d4ccf25091":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f537e1b0b0ae4e869f5d664ca40494d2","max":199,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fafeb3253cf94fa1842872797f0324f0","value":199}},"af191d3b2b5340c7988d4d4646c5f87e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1d26c44cb78459187564018cd046d91","placeholder":"​","style":"IPY_MODEL_4ac2986597964ba99fb73ed21bcac3ef","value":" 199/199 [00:00&lt;00:00,  7.34 examples/s]"}},"ac1da6ac3a3e425692cc74092e2b8f04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63f8d7b4a118482e92c4c050073c0883":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e21b8473ed8943c5bd307d1381738845":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f537e1b0b0ae4e869f5d664ca40494d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fafeb3253cf94fa1842872797f0324f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1d26c44cb78459187564018cd046d91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ac2986597964ba99fb73ed21bcac3ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4222a723a0245fb9d1d79e49de05cd2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dfe6231e9f694e9a965f8acf29a8ddf0","IPY_MODEL_928eda0cb20441638119190b3725d420","IPY_MODEL_d22bd2f19b8c40d4b7518edd66d077e5"],"layout":"IPY_MODEL_4c899ca79efd4018abf2d0be16979180"}},"dfe6231e9f694e9a965f8acf29a8ddf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bf5a4bf30d3452692a8673dd49f91b0","placeholder":"​","style":"IPY_MODEL_fb7254143a6b4282b95d1baa1b7b7820","value":"tokenizer_config.json: 100%"}},"928eda0cb20441638119190b3725d420":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c5af15c928643749e9615532e3c94f9","max":748,"min":0,"orientation":"horizontal","style":"IPY_MODEL_433b3bc216ff4c9f969077d8bb63181b","value":748}},"d22bd2f19b8c40d4b7518edd66d077e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_758db585e92c49739dc4dbf7d361e362","placeholder":"​","style":"IPY_MODEL_0b25058f0e6d4554a275a75ed4fd8172","value":" 748/748 [00:00&lt;00:00, 98.6kB/s]"}},"4c899ca79efd4018abf2d0be16979180":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bf5a4bf30d3452692a8673dd49f91b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb7254143a6b4282b95d1baa1b7b7820":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c5af15c928643749e9615532e3c94f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"433b3bc216ff4c9f969077d8bb63181b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"758db585e92c49739dc4dbf7d361e362":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b25058f0e6d4554a275a75ed4fd8172":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c15a2cd276b64f299e8e5c05647754b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e437f7eb18bd4a32b47a6abc4a158fd8","IPY_MODEL_7c1bf260a1cf47989420cf59468bbf32","IPY_MODEL_018752d5eb3643b3a5284f3b8eee7fc5"],"layout":"IPY_MODEL_204bb55e2961444b8b937252582aff4b"}},"e437f7eb18bd4a32b47a6abc4a158fd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f3609b6514f4794b7be3925034c2604","placeholder":"​","style":"IPY_MODEL_661d5e91ba03464d813ee6cb259f6836","value":"vocab.json: "}},"7c1bf260a1cf47989420cf59468bbf32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8490b088c74b41cd81c470783de8f1d2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee6297ecd3974fe5b2d197c3ac6ea400","value":1}},"018752d5eb3643b3a5284f3b8eee7fc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25ee3751318749ab84c5183dac1258c3","placeholder":"​","style":"IPY_MODEL_dfc0b29da91e4ab594b59e21c3438f1a","value":" 999k/? [00:00&lt;00:00, 30.6MB/s]"}},"204bb55e2961444b8b937252582aff4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f3609b6514f4794b7be3925034c2604":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"661d5e91ba03464d813ee6cb259f6836":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8490b088c74b41cd81c470783de8f1d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ee6297ecd3974fe5b2d197c3ac6ea400":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25ee3751318749ab84c5183dac1258c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfc0b29da91e4ab594b59e21c3438f1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5503bdc5033c4796bf76094ddeb8e796":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_211f20ec2acb48848aedbbe2e66b536b","IPY_MODEL_b556e60836c94ad09e982e8c7547627b","IPY_MODEL_9a25f7e6339a4705badbc541eab294e6"],"layout":"IPY_MODEL_a089e06b597345caada5ce241c230db2"}},"211f20ec2acb48848aedbbe2e66b536b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fd3569173554ae0ad75212568a6dc65","placeholder":"​","style":"IPY_MODEL_47fd3b79e9b446b2a5b37c4e03a29201","value":"merges.txt: "}},"b556e60836c94ad09e982e8c7547627b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e75712a142fe4acdb22e15d517e1edbd","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d42b25656444123ba5538cdab5d291e","value":1}},"9a25f7e6339a4705badbc541eab294e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da84238ea00d44bb9a7fcfc13b4bae54","placeholder":"​","style":"IPY_MODEL_9d57b126f2c848719f3290ad1ebe6756","value":" 456k/? [00:00&lt;00:00, 29.6MB/s]"}},"a089e06b597345caada5ce241c230db2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fd3569173554ae0ad75212568a6dc65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47fd3b79e9b446b2a5b37c4e03a29201":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e75712a142fe4acdb22e15d517e1edbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"5d42b25656444123ba5538cdab5d291e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da84238ea00d44bb9a7fcfc13b4bae54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d57b126f2c848719f3290ad1ebe6756":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5025bf2e1f647c4acc82c5ce8f187dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b2af17635524c1aa51209e56754d2b0","IPY_MODEL_abdf40f8ec974dac94c3a877927f1e77","IPY_MODEL_7eb783fd2b464845a98172c5a456721a"],"layout":"IPY_MODEL_63e9e8e66b8d424a8a9508753999feab"}},"5b2af17635524c1aa51209e56754d2b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c33d3630b7c404e8ad6296dbe2e30cf","placeholder":"​","style":"IPY_MODEL_6df4e81a46a2404a86240a7b9f1de508","value":"special_tokens_map.json: 100%"}},"abdf40f8ec974dac94c3a877927f1e77":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3db7654991f4cecbcaeb41f713a2f99","max":438,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d47001e2e7b840a980b63e2c9273b0ac","value":438}},"7eb783fd2b464845a98172c5a456721a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad1d21e723474cda852a607d6ceaefd0","placeholder":"​","style":"IPY_MODEL_2f4d6e7450d94c5d8a08339c7f696e8b","value":" 438/438 [00:00&lt;00:00, 61.2kB/s]"}},"63e9e8e66b8d424a8a9508753999feab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c33d3630b7c404e8ad6296dbe2e30cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6df4e81a46a2404a86240a7b9f1de508":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3db7654991f4cecbcaeb41f713a2f99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d47001e2e7b840a980b63e2c9273b0ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad1d21e723474cda852a607d6ceaefd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f4d6e7450d94c5d8a08339c7f696e8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6183ad6fd5b64850a3b3a6684943f89f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88d98f0994424f05acc1e6dc8ffc98f2","IPY_MODEL_fe40e9eeec7344c5a8268a741eda6191","IPY_MODEL_de66e41fd74d470db108afeeb964c4ab"],"layout":"IPY_MODEL_8676f8d1f985442fa6a6dd3f09c9d756"}},"88d98f0994424f05acc1e6dc8ffc98f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ec72664556348c4b2b2b3677d75ed75","placeholder":"​","style":"IPY_MODEL_291dad279218418d9f4f412349709f8b","value":"config.json: "}},"fe40e9eeec7344c5a8268a741eda6191":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a295f35378bc4c0493e8205ea43baf1f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73bb9e01e4f44d4da116d61436275e6e","value":1}},"de66e41fd74d470db108afeeb964c4ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ba604d541164f70af7d0a880e2c833c","placeholder":"​","style":"IPY_MODEL_cb33ce87224545e7ba54644cf5dfe0d5","value":" 1.00k/? [00:00&lt;00:00, 112kB/s]"}},"8676f8d1f985442fa6a6dd3f09c9d756":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ec72664556348c4b2b2b3677d75ed75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"291dad279218418d9f4f412349709f8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a295f35378bc4c0493e8205ea43baf1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"73bb9e01e4f44d4da116d61436275e6e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ba604d541164f70af7d0a880e2c833c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb33ce87224545e7ba54644cf5dfe0d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}